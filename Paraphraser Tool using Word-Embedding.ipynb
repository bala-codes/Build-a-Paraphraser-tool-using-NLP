{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation Assist - Upgraded.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bala-codes/Build-a-Paraphraser-tool-using-NLP/blob/master/Paraphraser%20Tool%20using%20Word-Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut5fl02B6j94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "7ba61502-e1b1-4655-89b5-ee54d9cb5eb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqSqeITg6e_y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "388f1b80-2b6a-4236-bfa2-6c1bdcc1332a"
      },
      "source": [
        "%%time\n",
        "# load the whole embedding into memory\n",
        "import numpy as np\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import string\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\"you've\" : \"you have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\", \"gonna\" : \"going to\", \"wanna\" : \"want to\", \"we've\" : \"we have\"}\n",
        "\n",
        "all_punctuations = string.punctuation + '‘’,:”][],' \n",
        "def punc_remover(raw_text):\n",
        "    no_punct = \"\".join([i for i in raw_text if i not in all_punctuations])\n",
        "    return no_punct\n",
        "def punc_remover2(raw_text):\n",
        "  for i in all_punctuations:\n",
        "    if(i in raw_text):\n",
        "      raw_text = raw_text.replace(i,\" \")\n",
        "  return raw_text\n",
        "\n",
        "def punc_rem(raw_text):\n",
        "  import re\n",
        "  s = re.sub(r'[^\\w\\s]','',raw_text)\n",
        "  return s\n",
        "\n",
        "def contraction_mapper(newString):\n",
        "  newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.lower().split(\" \")])  \n",
        "  return newString\n",
        "\n",
        "def genword(embedding):\n",
        "    words = embedding.split()\n",
        "    genwords = set(nltk.corpus.words.words())\n",
        "    gen_words = \" \".join([i for i in words if i.lower() in genwords])\n",
        "    return gen_words\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "genwords = set(nltk.corpus.words.words())\n",
        "\n",
        "def lem(words):\n",
        "  return \" \".join([lemmer.lemmatize(word,'v') for word in words.split()])\n",
        "def portalstem(words):\n",
        "  return \" \".join([porter.stem(word) for word in words.split()])\n",
        "def lancastem(words):\n",
        "  return \" \".join([lancaster.stem(word) for word in words.split()])\n",
        "def stopword_remover(embedding):\n",
        "    words = embedding.split()\n",
        "    no_stp_words = \" \".join([i for i in words if i not in stopwords.words('english')])\n",
        "    return no_stp_words\n",
        "def digit_remover(embedding):\n",
        "  words = embedding.split()\n",
        "  return ' '.join([i for i in words if not i.isdigit()]) \n",
        "\n",
        "embeddings_index = dict()\n",
        "embedding_dims = []\n",
        "\n",
        "f = open('/content/drive/My Drive/Colab Notebooks/TEXT EMBEDDINGS FILES/Glove_Embedding/glove.6B.300d.txt')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "################################################################################\n",
        "from scipy import spatial\n",
        "def find_closest_embeddings(embedding):\n",
        "    return sorted(embeddings_index.keys(), key=lambda word: spatial.distance.euclidean(embeddings_index[word], embedding))\n",
        "\n",
        "'''\n",
        "def similar_words():\n",
        "  embedding = input(str(\"Enter the Passage Here : \"))\n",
        "  embedding = lem(stopword_remover(digit_remover(punc_remover2(contraction_mapper(embedding.lower())))))\n",
        "\n",
        "  print()\n",
        "  if len(embedding.split()) == 1:\n",
        "    print(color.BOLD + embedding + color.END)\n",
        "    print(find_closest_embeddings(embeddings_index[embedding])[:15])\n",
        "  else:\n",
        "    x = len(embedding.split())\n",
        "    for i in range(x):\n",
        "      print(color.BOLD + embedding.split()[i] + color.END)\n",
        "      print(find_closest_embeddings(embeddings_index[embedding.split()[i]])[:15])\n",
        "      print()\n",
        " \n",
        "def similar_passage():\n",
        "  embedding = input(str(\"Enter the Passage Here : \"))\n",
        "  temp = embedding\n",
        "  embedding = lem(stopword_remover(digit_remover(punc_remover2(contraction_mapper(embedding.lower())))))\n",
        "\n",
        "  wordict = {}\n",
        "\n",
        "  if len(embedding.split()) == 1:\n",
        "    #print(color.BOLD + embedding + color.END)\n",
        "    #print(find_closest_embeddings(embeddings_index[embedding])[:15])\n",
        "    wordict[embedding] = find_closest_embeddings(embeddings_index[embedding])[1:4]\n",
        "\n",
        "  else:\n",
        "    x = len(embedding.split())\n",
        "    for i in range(x):\n",
        "      if len(embedding.split()[i]) > 3:\n",
        "        #print(color.BOLD + embedding.split()[i] + color.END)\n",
        "        #print(find_closest_embeddings(embeddings_index[embedding.split()[i]])[:5])\n",
        "        wordict[embedding.split()[i]] = find_closest_embeddings(embeddings_index[embedding.split()[i]])[1:4]\n",
        "        \n",
        "      else:\n",
        "        continue\n",
        "  import random\n",
        "  wordict2 = {}\n",
        "  for k,v in wordict.items():\n",
        "    wordict2[k] = wordict[k][random.randint(0, 1)]\n",
        "\n",
        "  def words_swapper(newString):\n",
        "    newString = ' '.join([wordict2[t] if t in wordict2 else t for t in newString.split(\" \")])  \n",
        "    return newString\n",
        "  print()\n",
        "  print(\"ORIGINAL TEXT \\n\\n\",temp)\n",
        "  print()\n",
        "  x = words_swapper(temp)\n",
        "  print(\"MANIPULATED TEXT \\n\\n\",x)\n",
        "  print()\n",
        "'''\n",
        "\n",
        "def similar_words():\n",
        "  words = input(str(\"Enter the Passage Here : \"))\n",
        "  words = (stopword_remover(digit_remover(punc_rem(punc_remover2(contraction_mapper(words.lower())))))) #lem removed\n",
        "\n",
        "  embedding = []\n",
        "  embedding = words\n",
        "\n",
        "  \n",
        "  if len(embedding.split()) == 1:\n",
        "    print(color.BOLD + str(embedding) + color.END)\n",
        "    temp1 = []\n",
        "    try:\n",
        "      temp1 = find_closest_embeddings(embeddings_index[embedding])[1:]  \n",
        "    except:\n",
        "      print(\"Alternate Word Not found for :\",embedding.split()[i]) \n",
        "    y = []\n",
        "    for k in range(len(temp1)):\n",
        "      if temp1[k].isalpha() == True:\n",
        "        y.append(temp1[k])\n",
        "\n",
        "    y = [w for w in y if w.lower() not in stopwords.words('english')] #\n",
        "    y = [w for w in y if not w.isdigit()] #\n",
        "    print(y[1:15])\n",
        "\n",
        "  else: \n",
        "    for i in range(len(embedding.split())):\n",
        "      print(color.BOLD + embedding.split()[i] + color.END)\n",
        "      temp2 = []\n",
        "      try:\n",
        "        temp2 = find_closest_embeddings(embeddings_index[embedding.split()[i]])[1:]   \n",
        "      except:\n",
        "        print(\"Alternate Word Not found for :\",embedding.split()[i]) \n",
        "      y = []\n",
        "      for j in range(len(temp2)):\n",
        "        if temp2[j].isalpha() == True:\n",
        "          y.append(temp2[j])\n",
        "      \n",
        "      y = [w for w in y if w.lower() not in stopwords.words('english')] #\n",
        "      y = [w for w in y if not w.isdigit()] #\n",
        "      print(y[1:15])\n",
        "      print()\n",
        "\n",
        "def similar_passage():\n",
        "  embedding = input(str(\"Enter the Passage Here : \"))\n",
        "  temp = contraction_mapper(embedding)                # contraction_mapper added\n",
        "  embedding = (stopword_remover(digit_remover(punc_rem(punc_remover2(contraction_mapper(embedding.lower())))))) #lem removed\n",
        "\n",
        "  wordict = {}\n",
        "  if len(embedding.split()) == 1:\n",
        "    print(\"The input text contains a single word, kindly use similar_words or simply enter your text again \")\n",
        "    similar_words()\n",
        "\n",
        "  else:\n",
        "    wordict = {}\n",
        "    x = len(embedding.split())\n",
        "    for i in range(x):\n",
        "      if len(embedding.split()[i]) > 3:\n",
        "        y = []\n",
        "        try:\n",
        "          temp2 = []\n",
        "          temp2 = find_closest_embeddings(embeddings_index[embedding.split()[i]])[1:5]   # [1:4] \n",
        "        except:\n",
        "          print(\"Alternate Word Not found for :\",embedding.split()[i])\n",
        "\n",
        "        #y = []\n",
        "        if len(temp2) != (0 or 1) :    # != 0\n",
        "          for j in range(len(temp2)):\n",
        "            if temp2[j].isalpha() == True:\n",
        "              y.append(temp2[j])\n",
        "            else:\n",
        "              continue\n",
        "\n",
        "          y = [w for w in y if w.lower() not in stopwords.words('english')] \n",
        "          y = [w for w in y if not w.isdigit()]\n",
        "\n",
        "          if len(y) != 1 and len(y) != 0 : # from 0 -> 0 and 1\n",
        "            #print(y)\n",
        "            wordict[embedding.split()[i]] = y\n",
        "          else:\n",
        "            continue\n",
        "        else:\n",
        "          continue\n",
        "      else:\n",
        "        continue\n",
        "  import random\n",
        "  wordict2 = {}\n",
        "  for k in wordict.items():\n",
        "    wordict2[k[0]] = k[1][random.randint(0,1)]\n",
        "\n",
        "  def words_swapper(newString):\n",
        "    newString = ' '.join([wordict2[t] if t in wordict2 else t for t in newString.split(\" \")])  \n",
        "    return newString\n",
        "  print()\n",
        "  print(color.BOLD + \"ORIGINAL TEXT \\n\\n\" + color.END,temp)\n",
        "  print()\n",
        "  x = words_swapper(temp)\n",
        "  print(color.BOLD + \"MANIPULATED TEXT \\n\\n\" + color.END,x)\n",
        "  print() \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "Loaded 400000 word vectors.\n",
            "CPU times: user 29 s, sys: 1.13 s, total: 30.1 s\n",
            "Wall time: 34.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awB1vFtq9YC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "f755f2fb-b171-49cb-8f70-b8c542ff1001"
      },
      "source": [
        "%%time\n",
        "similar_passage()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Passage Here : Enter the Passage Here :  complementnb implements the complement naive bayes (cnb) algorithm. cnb is an adaptation of the standard multinomial naive bayes (mnb) algorithm that is particularly suited for imbalanced data sets. specifically, cnb uses statistics from the complement of each class to compute the model’s weights. the inventors of cnb show empirically that the parameter estimates for cnb are more stable than those for mnb. further, cnb regularly outperforms mnb (often by a considerable margin) on text classification tasks. the procedure for calculating the weights is as follows:         where the summations are over all documents  not in class ,  is either the count or tf-idf value of term  in document ,   is a smoothing hyperparameter like that found in mnb, and  . the second normalization addresses the tendency for longer documents to dominate parameter estimates in mnb. the classification rule is:    \t  i.e., a document is assigned to the class that is the poorest complement match.\n",
            "Alternate Word Not found for : complementnb\n",
            "Alternate Word Not found for : hyperparameter\n",
            "\n",
            "\u001b[1mORIGINAL TEXT \n",
            "\n",
            "\u001b[0m enter the passage here :  complementnb implements the complement naive bayes (cnb) algorithm. cnb is an adaptation of the standard multinomial naive bayes (mnb) algorithm that is particularly suited for imbalanced data sets. specifically, cnb uses statistics from the complement of each class to compute the model’s weights. the inventors of cnb show empirically that the parameter estimates for cnb are more stable than those for mnb. further, cnb regularly outperforms mnb (often by a considerable margin) on text classification tasks. the procedure for calculating the weights is as follows:         where the summations are over all documents  not in class ,  is either the count or tf-idf value of term  in document ,   is a smoothing hyperparameter like that found in mnb, and  . the second normalization addresses the tendency for longer documents to dominate parameter estimates in mnb. the classification rule is:    \t  i.e., a document is assigned to the class that is the poorest complement match.\n",
            "\n",
            "\u001b[1mMANIPULATED TEXT \n",
            "\n",
            "\u001b[0m entered the provision here :  complementnb implements the complemented naïve grihn (cnb) algorithm. cnb is an adapted of the standards lemniscate naïve grihn (mnb) computation that is especially ideally for imbalanced information sets. specifically, cnb using statistic from the complemented of each example to calculate the model’s weights. the inventions of cnb shows validated that the parameter estimate for cnb are more maintain than those for mnb. further, cnb frequently outshines mnb (often by a substantial margin) on document furthermore tasks. the procedures for calculation the weight is as follows:         where the summations are over all papers  not in example ,  is rather the counts or tf-idf valued of terms  in documents ,   is a smoothing hyperparameter well that discovered in mnb, and  . the fourth normalizing address the propensity for though papers to dominating parameter estimate in mnb. the furthermore exception is:    \t  i.e., a documents is assignment to the example that is the impoverished complemented match.\n",
            "\n",
            "CPU times: user 10min 14s, sys: 390 ms, total: 10min 15s\n",
            "Wall time: 10min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbz-YEZUIWzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "5d25ea60-3ac1-405b-c592-550fbe23129e"
      },
      "source": [
        "%%time\n",
        "similar_words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Passage Here : vision perspective', 'mind', 'indeed', 'view', 'visionary', 'aim', contras \n",
            "\u001b[1mvision\u001b[0m\n",
            "['perspective', 'mind', 'indeed', 'view', 'visionary', 'aim', 'contrast', 'focus', 'realized', 'likewise', 'concept', 'aims', 'unfortunately', 'realizing']\n",
            "\n",
            "\u001b[1mperspective\u001b[0m\n",
            "['context', 'perspectives', 'view', 'importantly', 'aspect', 'suggests', 'approach', 'regard', 'thinking', 'indeed', 'furthermore', 'aspects', 'instance', 'sort']\n",
            "\n",
            "\u001b[1mmind\u001b[0m\n",
            "['else', 'something', 'sort', 'things', 'thought', 'kind', 'maybe', 'thing', 'seems', 'indeed', 'everything', 'really', 'actually', 'certainly']\n",
            "\n",
            "\u001b[1mindeed\u001b[0m\n",
            "['moreover', 'certainly', 'nevertheless', 'likewise', 'unfortunately', 'though', 'nonetheless', 'instance', 'actually', 'seems', 'perhaps', 'surely', 'yet', 'hardly']\n",
            "\n",
            "\u001b[1mview\u001b[0m\n",
            "['viewed', 'regard', 'indeed', 'fact', 'perspective', 'viewpoint', 'moreover', 'rather', 'instance', 'nevertheless', 'example', 'though', 'likewise', 'contrast']\n",
            "\n",
            "\u001b[1mvisionary\u001b[0m\n",
            "['visionaries', 'exemplified', 'embodied', 'iconoclastic', 'inspiring', 'piyanart', 'srivalo', 'bulletinyyy', 'ooooooooooooooooooooooooooooooooooooooo', 'innovative', 'genius', 'drohs', 'pioneering', 'pyoot']\n",
            "\n",
            "\u001b[1maim\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkoGdL9V0Z3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}